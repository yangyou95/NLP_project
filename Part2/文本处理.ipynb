{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import jieba.analyse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_path = \"C:\\\\Users\\\\Name\\\\PycharmProjects\\\\NLP_project\\\\resources\\\\stopwords-master\\\\百度停用词表.txt\"\n",
    "# stop_words_path = \"C:\\\\Users\\\\Name\\\\PycharmProjects\\\\NLP_project\\\\resources\\\\stopwords-master\\\\哈工大停用词表.txt\"\n",
    "# stop_words_path = \"/home/yang/PycharmProjects/NLP_projects/resources/stopwords-master/哈工大停用词表.txt\"\n",
    "stop_words_path = \"/home/yang/PycharmProjects/NLP_projects/resources/stopwords-master/中文停用词表.txt\"\n",
    "stop_words = pd.read_csv(stop_words_path,index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stop_words = stop_words['stopword'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"C:\\\\Users\\\\Name\\\\PycharmProjects\\\\NLP_project\\\\resources\\\\面试-训练测试集.xlsx\"\n",
    "data_path = \"/home/yang/PycharmProjects/NLP_projects/resources/面试-训练测试集.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "\n",
    "# title_data = data['title']\n",
    "# content_data = data['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.定义文本预处理函数\n",
    "\n",
    "* 去停用词，分词\n",
    "* 参数 titles, contents, sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(s):\n",
    "    s = str(s)\n",
    "    if s.count('.') ==1:\n",
    "        left = s.split('.')[0]\n",
    "        right = s.split('.')[1]\n",
    "        if right.isdigit():\n",
    "            if left.count('-')==1 and left.startswith('-'):\n",
    "                num = left.split['-'][-1]\n",
    "                if num.isdigit():\n",
    "                    return True\n",
    "            elif left.isdigit():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_percents(s):\n",
    "    s = str(s)\n",
    "    if s.count('.') ==1:\n",
    "        left = s.split('.')[0]\n",
    "        right = s.split('.')[1]\n",
    "        right = right.split('%')[0]\n",
    "        if right.isdigit():\n",
    "            if left.count('-')==1 and left.startswith('-'):\n",
    "                num = left.split['-'][-1]\n",
    "                if num.isdigit():\n",
    "                    return True\n",
    "            elif left.isdigit():\n",
    "                return True\n",
    "            \n",
    "    elif s.count(\".\") != 1:\n",
    "        left = s.split('%')[0]\n",
    "        if left.isdigit():\n",
    "            return True\n",
    "               \n",
    "    return False\n",
    "\n",
    "\n",
    "def preprocess_text(title, content, sentences, label):\n",
    "    \n",
    "#     处理标题\n",
    "    title_info = jieba.lcut(title)\n",
    "\n",
    "    title_info = [v for v in title_info if not str(v).isdigit()]#去数字\n",
    "    title_info = list(filter(lambda x:x.strip(), title_info))   #去左右空格\n",
    "    \n",
    "\n",
    "    title_info = list(filter(lambda x:len(x)>1, title_info)) #长度为1的字符\n",
    "    title_info = list(filter(lambda x:x not in stop_words, title_info)) #去掉停用词\n",
    "    \n",
    "#     print(\"标题信息是：\" ,title_info)\n",
    "#     sentences.append(segs)\n",
    "#     sentences.append((\" \".join(segs), label))# 打标签\n",
    "            \n",
    "#     处理文章内容        \n",
    "    segs=jieba.lcut(content)\n",
    "    segs = [v for v in segs if not str(v).isdigit()]#去数字\n",
    "    segs = [v for v in segs if not is_float(v)] #去小数\n",
    "    segs = [v for v in segs if not is_percents(v)] #去百分数\n",
    "\n",
    "    segs = list(filter(lambda x:x.strip(), segs))   #去左右空格\n",
    "    segs = list(filter(lambda x:len(x)>1, segs)) #长度为1的字符\n",
    "    segs = list(filter(lambda x:x not in stop_words, segs)) #去掉停用词\n",
    "    \n",
    "#     print(\"内容信息是:\", segs)\n",
    "    segs = title_info + segs\n",
    "    sentences.append((\" \".join(segs), label))# 打标签\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "#     print(row['title'], row['content'])\n",
    "    title = row['title']\n",
    "    content = row['content']\n",
    "    label = row['label']\n",
    "    preprocess_text(title, content, sentences, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "文章分词: 赵东 交易所 BTC 周线 放量 现在 反弹 牛市 DGroup 创始人 赵东 发微 博称 激动 交易所 BTC 周线 放量 现在 反弹 牛市 牛会来 现在 耐心\n",
      "文章标签： 正面\n",
      "-------\n",
      "文章分词: 世界 行情 BTC 小幅 回调 美元 附近 宽幅 震荡 BTC 昨日 高涨 美元 目前 小幅 回调 主流 多数 小幅 回调 BTC 在币安现 美元 24h 跌幅 全球 数字 货币 市场 总价值 亿美元 24h 成交量 亿美元 BTC 全球 均价 美元 三大 主流 交易所 火币 现报 美元 OKEx 现报 美元 币安现 美元 主流 数字 货币 表现 ETH 暂报 美元 XRP 暂报 美元 BCH 暂报 美元 LTC 暂报 美元 ETC 暂报 美元 EOS 暂报 美元 小时 行情 市值 排名 前百 币种 涨幅 前三为 NULS NANO XTZ 跌幅 前三为 FT WAX PAI 世界 行情 概念 板块 涨幅 前三为 DAG 技术 合约 工具 跨链 跌幅 前三为 人工智能 身份验证 联网 涨跌幅 24h 计算\n",
      "文章标签： 正面\n",
      "-------\n",
      "文章分词: BTC 短时 跌破 美元 世界 行情 BTC 刚刚 快速 下跌 短时 跌破 美元 最低 跌至 美元 略有 回升 BTC 火币 现报 美元 今日 跌幅 实时 行情 异动 提醒 点击 查看 原文 开启 智能 盯盘\n",
      "文章标签： 负面\n",
      "-------\n",
      "文章分词: 过去 小时 推特 讨论 BTC 排名 第一 ETH XRP 排名 三位 CoinTrendz com 数据 显示 过去 小时 推特 讨论 排行 BTC 讨论 排名 第一 ETH 排名 第二位 XRP 排名 第三位 讨论 排名 四至 十位 LTC BAT ADA ATT TRX XVG BNB\n",
      "文章标签： 负面\n",
      "-------\n",
      "文章分词: Coindesk 分析 BTC 美元 均线 支撑 有效 Coindesk 分析 BTC 未来 几天 可能 回落 美元 第一季度 引领 市场 走高 LTC 显示 疲软 迹象 中旬 月底 LTC BTC 几乎 走势 相同 BTC 多头 空头 比率 首次 跌至 以下 表明 看跌 情绪 日益加剧 价格 回调 看起来 可能 BTC 价格 借助 上升 移动 平均线 目前 美元 反弹 下跌 美元 情况 将会 减弱\n",
      "文章标签： 负面\n",
      "-------\n",
      "文章分词: 分析 BTC 期货 多空 实力 反转 市场 情绪 逐渐 冷静 TokenGazer 数据分析 显示 截止 点整 BTC 价格 USDT 收益 回报率 ROI 环比 昨日 跌幅 RSI 指数 处于 正常 区间 期货 方面 Bitfinex BitMEX 总多 单量 总空 单量 相较 此前 情况 市场 看好 情绪 已然 冷静 空头 势力 逐渐 攀升\n",
      "文章标签： 负面\n",
      "-------\n",
      "文章分词: 过去 小时 推特 讨论 BTC 排名 第一 ETH XRP 排名 三位 CoinTrendz com 数据 显示 过去 小时 推特 讨论 排行 BTC 讨论 排名 第一 ETH 排名 第二位 XRP 排名 第三位 讨论 排名 四至 十位 LTC BAT ADA ATT TRX XVG BNB\n",
      "文章标签： 中性\n",
      "-------\n",
      "文章分词: BTC 链上 出现 价值 万美元 大额 转账 世界 监测 北京 时间 BTC 链上 出现 大额 转账 357R3FeNmySYeHuRfyhFd6nMwzoLDdjfwV 钱包 地址 3NmHmQte2rP8pS54U3B8LPYQKkpG1pFF69 地址 转账 BTC 价值 万美元\n",
      "文章标签： 正面\n",
      "-------\n",
      "文章分词: BTC ETH XRP 主流 普跌 世界 行情 BTC ETH XRP 主流 币种 短时 持续 下跌 BTC 现报 美元 24h 跌幅 ETH 现报 美元 24h 跌幅 XRP 现报 美元 24h 跌幅 主流 普跌 实时 行情 异动 提醒 点击 查看 原文 开启 智能 盯盘\n",
      "文章标签： 负面\n",
      "-------\n",
      "文章分词: BTC 短时 再次 跌破 美元 世界 行情 BTC 短时 持续 下跌 跌破 美元 现报 美元 跌幅 实时 行情 异动 提醒 点击 查看 原文 开启 智能 盯盘\n",
      "文章标签： 负面\n"
     ]
    }
   ],
   "source": [
    "# random.shuffle(sentences)\n",
    "for sentence in sentences[:10]:\n",
    "    print(\"-------\")\n",
    "    print(\"文章分词:\", sentence[0])\n",
    "    print(\"文章标签：\", sentence[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 抽取词向量特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 抽取特征， 定义词袋模型\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# vec = CountVectorizer(\n",
    "#     analyzer='word', # tokenise by character ngrams\n",
    "#     ngram_range=(1,4),  # use ngrams of size 1 and 2\n",
    "#     max_features=1000,  # keep the most common 1000 ngrams\n",
    "# )\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    ngram_range=(1,2),  # use ngrams of size 1 and 2\n",
    "    max_features=500,  # keep the most common 1000 ngrams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 语料切分\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.35, random_state=1256)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=500,\n",
       "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 把训练数据转为词袋模型\n",
    "vec.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.算法建模与模型训练 （贝叶斯）\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# classifier = MultinomialNB()\n",
    "# classifier.fit(vec.transform(x_train), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 评估结果 AUC值\n",
    "# print(classifier.score(vec.transform(x_test), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM模型评价，训练集正确率 0.8796296296296297\n",
      "SVM模型评价，测试集正确率 0.8685714285714285\n"
     ]
    }
   ],
   "source": [
    "# 4.算法建模与模型训练 （SVM）\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# svm = SVC(kernel='linear')\n",
    "svm = SVC(C=1.0,kernel='linear',gamma='auto')\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "\n",
    "print(\"SVM模型评价，训练集正确率\",svm.score(vec.transform(x_train), y_train))\n",
    "print(\"SVM模型评价，测试集正确率\",svm.score(vec.transform(x_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目前准确率在测试集上为80%左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神经网络模型评价，训练集正确率 0.9969135802469136\n",
      "神经网络模型评价, 测试集正确率: 0.8171428571428572\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neural_network as sk_nn\n",
    "model = sk_nn.MLPClassifier(activation='relu',solver='lbfgs',alpha=0.0001,learning_rate='adaptive',learning_rate_init=0.001,max_iter=2000)\n",
    "model.fit(vec.transform(x_train),y_train)\n",
    "\n",
    "print(\"神经网络模型评价，训练集正确率\", model.score(vec.transform(x_train), y_train))\n",
    "\n",
    "acc=model.score(vec.transform(x_test),y_test) #根据给定数据与标签返回正确率的均值\n",
    "print('神经网络模型评价, 测试集正确率:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
