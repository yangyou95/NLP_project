{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import jieba.analyse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_path = \"C:\\\\Users\\\\Name\\\\PycharmProjects\\\\NLP_project\\\\resources\\\\stopwords-master\\\\百度停用词表.txt\"\n",
    "# stop_words_path = \"C:\\\\Users\\\\Name\\\\PycharmProjects\\\\NLP_project\\\\resources\\\\stopwords-master\\\\哈工大停用词表.txt\"\n",
    "stop_words_path = \"/home/yang/PycharmProjects/NLP_projects/resources/stopwords-master/哈工大停用词表.txt\"\n",
    "# stop_words_path = \"/home/yang/PycharmProjects/NLP_projects/resources/stopwords-master/中文停用词表.txt\"\n",
    "# stop_words_path = \"/home/yang/PycharmProjects/NLP_projects/resources/stopwords-master/modified_stopwords.txt\"\n",
    "\n",
    "# stop_words_path = \"/home/yang/PycharmProjects/NLP_project/resources/stopwords-master/中文停用词表.txt\"\n",
    "# stop_words_path = \"/home/yang/PycharmProjects/NLP_project/resources/stopwords-master/百度停用词表.txt\"\n",
    "# stop_words_path = \"/home/yang/PycharmProjects/NLP_project/resources/stopwords-master/modified_stopwords.txt\"\n",
    "# stop_words_path = \"/home/yang/PycharmProjects/NLP_project/resources/stopwords-master/四川大学机器智能实验室停用词库.txt\"\n",
    "\n",
    "stop_words = pd.read_csv(stop_words_path,index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stop_words = stop_words['stopword'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"C:\\\\Users\\\\Name\\\\PycharmProjects\\\\NLP_project\\\\resources\\\\面试-训练测试集.xlsx\"\n",
    "data_path = \"/home/yang/PycharmProjects/NLP_projects/resources/面试-训练测试集.xlsx\"\n",
    "predict_data_path = \"/home/yang/PycharmProjects/NLP_projects/resources/面试-待预测集.xlsx\"\n",
    "# data_path = \"/home/yang/PycharmProjects/NLP_project/resources/面试-训练测试集.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "predict_data = pd.read_excel(predict_data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.定义文本预处理函数\n",
    "\n",
    "* 去无关字符，去停用词，分词\n",
    "* 参数 titles, contents, sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(s):\n",
    "    s = str(s)\n",
    "    if s.count('.') ==1:\n",
    "        left = s.split('.')[0]\n",
    "        right = s.split('.')[1]\n",
    "        if right.isdigit():\n",
    "            if left.count('-')==1 and left.startswith('-'):\n",
    "                num = left.split['-'][-1]\n",
    "                if num.isdigit():\n",
    "                    return True\n",
    "            elif left.isdigit():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def is_percents(s):\n",
    "    s = str(s)\n",
    "    if s.count('.') ==1:\n",
    "        left = s.split('.')[0]\n",
    "        right = s.split('.')[1]\n",
    "        right = right.split('%')[0]\n",
    "        if right.isdigit():\n",
    "            if left.count('-')==1 and left.startswith('-'):\n",
    "                num = left.split['-'][-1]\n",
    "                if num.isdigit():\n",
    "                    return True\n",
    "            elif left.isdigit():\n",
    "                return True\n",
    "            \n",
    "    elif s.count(\".\") != 1:\n",
    "        left = s.split('%')[0]\n",
    "        if left.isdigit():\n",
    "            return True\n",
    "               \n",
    "    return False\n",
    "\n",
    "def is_all_chinese(strs):\n",
    "    for _char in strs:\n",
    "        if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocess_text(title, content, sentences, label):\n",
    "    \n",
    "#     处理标题\n",
    "    title_info = jieba.lcut(title)\n",
    "\n",
    "    title_info = [v for v in title_info if not str(v).isdigit()]#去数字\n",
    "    title_info = list(filter(lambda x:x.strip(), title_info))   #去左右空格\n",
    "    \n",
    "\n",
    "    title_info = list(filter(lambda x:len(x)>1, title_info)) #长度为1的字符\n",
    "    title_info = list(filter(lambda x:x not in stop_words, title_info)) #去掉停用词\n",
    "            \n",
    "#     处理文章内容        \n",
    "    segs=jieba.lcut(content)\n",
    "    segs = [v for v in segs if not str(v).isdigit()]#去数字\n",
    "    segs = [v for v in segs if not is_float(v)] #去小数\n",
    "    segs = [v for v in segs if not is_percents(v)] #去百分数\n",
    "\n",
    "    segs = list(filter(lambda x:x.strip(), segs))   #去左右空格\n",
    "    segs = list(filter(lambda x:len(x)>1, segs)) #长度为1的字符\n",
    "    segs = list(filter(lambda x:x not in stop_words, segs)) #去掉停用词\n",
    "    \n",
    "    \n",
    "#     print(\"内容信息是:\", segs)\n",
    "    segs = title_info + segs  \n",
    "    segs = [v for v in segs if is_all_chinese(str(v))] #去字母词\n",
    "    \n",
    "    sentences.append((\" \".join(segs), label))# 打标签\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "#     print(row['title'], row['content'])\n",
    "    title = row['title']\n",
    "    content = row['content']\n",
    "    label = row['label']\n",
    "    preprocess_text(title, content, sentences, label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(sentences)\n",
    "# for sentence in sentences[:10]:\n",
    "#     print(\"-------\")\n",
    "#     print(\"文章分词:\", sentence[0])\n",
    "#     print(\"文章标签：\", sentence[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 抽取词向量特征\n",
    "\n",
    "* 测试了两种提取方法，CountVectorizer词频提取和TfidfVectorizer的td-idf特性提取\n",
    "* CountVectorizer提取词频特征后，在训练集上有明显过拟合现象\n",
    "* Tf-idf模型能更有效避免噪声词的干扰，在测试集上表现更好，这里用Tf-idf来构建词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 抽取特征， 定义词袋模型\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# vec = CountVectorizer(\n",
    "#     analyzer='word', # tokenise by character ngrams\n",
    "#     ngram_range=(1,2),  # use ngrams of size 1 and 2\n",
    "#     max_features=1000,  # keep the most common 1000 ngrams\n",
    "# )\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    ngram_range=(1,3),  # use ngrams of size 1 and 2\n",
    "    max_features= None,  # keep the most common 1000 ngrams\n",
    "    use_idf=1,smooth_idf=1,sublinear_tf=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 语料切分, 随机分出测试集用于验证，比例70%训练集，30%测试集\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.3, random_state=1256)\n",
    "# print(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 把训练数据转为词袋模型\n",
    "vec.fit(x_train)\n",
    "x_train_data = vec.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多项贝叶斯模型评价，训练集正确率: 0.9054441260744985\n"
     ]
    }
   ],
   "source": [
    "# 4.算法建模与模型训练 （贝叶斯）\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train_data, y_train)\n",
    "# 5. 评估结果 AUC值\n",
    "print(\"多项贝叶斯模型评价，训练集正确率:\",classifier.score(vec.transform(x_train), y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM模型评价，训练集正确率 0.9684813753581661\n"
     ]
    }
   ],
   "source": [
    "# 4.算法建模与模型训练 （SVM）\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "# svm = SVC(kernel='linear')\n",
    "svm = SVC(C=1.0,kernel='linear',gamma='auto')\n",
    "svm.fit(x_train_data, y_train)\n",
    "\n",
    "print(\"SVM模型评价，训练集正确率\",svm.score(x_train_data, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神经网络模型评价，训练集正确率 0.9856733524355301\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neural_network as sk_nn\n",
    "model = sk_nn.MLPClassifier(activation='tanh',solver='sgd',alpha=0.0001,learning_rate='adaptive',learning_rate_init=0.01,max_iter=2000)\n",
    "model.fit(x_train_data,y_train)\n",
    "\n",
    "print(\"神经网络模型评价，训练集正确率\", model.score(x_train_data, y_train))\n",
    "\n",
    "acc=model.score(vec.transform(x_test),y_test) #根据给定数据与标签返回正确率的均值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.检验模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多项贝叶斯模型评价，测试集正确率: 0.8533333333333334\n",
      "SVM模型评价，测试集正确率 0.9133333333333333\n",
      "神经网络模型评价, 测试集正确率: 0.9133333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"多项贝叶斯模型评价，测试集正确率:\",classifier.score(vec.transform(x_test), y_test))\n",
    "print(\"SVM模型评价，测试集正确率\",svm.score(vec.transform(x_test), y_test))\n",
    "print('神经网络模型评价, 测试集正确率:',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目前准确率最高的模型是神经网络，在测试集上为92%左右， SVM模型在测试集上准确率91%左右，但训练速度更快。这里我们保存神经网络的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# 保存模型\n",
    "joblib.dump(model,'mlp.model') \n",
    "# 加载模型\n",
    "mlp_model=joblib.load('mlp.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 预测分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['正面' '正面' '负面' '负面' '负面' '负面' '负面' '正面' '负面' '负面' '负面' '负面' '正面' '正面'\n",
      " '正面' '正面' '正面' '负面' '正面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面'\n",
      " '正面' '中性' '正面' '正面' '负面' '正面' '正面' '正面' '正面' '负面' '负面' '负面' '正面' '负面'\n",
      " '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '负面' '正面' '正面'\n",
      " '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面'\n",
      " '正面' '正面' '正面' '负面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面' '正面' '正面'\n",
      " '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '负面' '正面'\n",
      " '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '负面' '正面'\n",
      " '正面' '正面' '正面' '中性' '正面' '负面' '负面' '正面' '中性' '正面' '正面' '正面' '负面' '正面'\n",
      " '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '中性' '正面' '负面' '正面'\n",
      " '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '负面'\n",
      " '正面' '负面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面'\n",
      " '正面' '负面' '正面' '负面' '负面' '正面' '中性' '正面' '正面' '正面' '正面' '正面' '正面' '负面'\n",
      " '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面'\n",
      " '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面'\n",
      " '正面' '负面' '负面' '正面' '负面' '负面' '中性' '正面' '正面' '负面' '正面' '正面' '正面' '负面'\n",
      " '正面' '负面' '正面' '正面' '负面' '负面' '正面' '正面' '正面' '负面' '正面' '负面' '正面' '正面'\n",
      " '正面' '正面' '负面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面'\n",
      " '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '负面' '负面' '正面' '正面'\n",
      " '正面' '正面' '正面' '负面' '正面' '中性' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面'\n",
      " '正面' '正面' '负面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '负面' '正面' '正面' '正面'\n",
      " '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '负面' '正面' '负面' '负面'\n",
      " '正面' '负面' '负面' '正面' '正面' '正面' '负面' '负面' '正面' '正面' '正面' '正面' '正面' '正面'\n",
      " '正面' '正面' '正面' '正面' '负面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面'\n",
      " '正面' '正面' '正面' '正面' '正面' '负面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '负面'\n",
      " '正面' '正面' '正面' '正面' '正面' '负面' '负面' '正面' '正面' '正面' '正面' '负面' '负面' '正面'\n",
      " '负面' '负面' '正面' '负面' '负面' '正面' '正面' '负面' '正面' '正面' '负面' '正面' '负面' '正面'\n",
      " '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面'\n",
      " '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面'\n",
      " '正面' '正面' '中性' '正面' '负面' '正面' '负面' '正面' '负面' '正面' '正面' '负面' '正面' '负面'\n",
      " '负面' '负面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面' '正面'\n",
      " '正面' '正面' '正面' '中性' '正面' '负面' '正面' '正面' '负面' '负面' '负面' '负面' '正面' '正面'\n",
      " '正面' '正面' '正面' '负面' '负面' '正面' '中性' '正面' '正面' '正面' '正面' '正面' '正面' '正面'\n",
      " '正面' '正面' '负面' '正面' '正面' '负面' '正面' '负面' '负面' '正面' '正面' '负面' '负面' '负面'\n",
      " '正面' '正面' '正面' '正面' '正面' '负面' '正面' '正面' '正面' '正面' '正面' '正面' '负面' '正面'\n",
      " '正面' '正面' '正面' '负面' '正面' '负面' '负面' '负面' '正面']\n"
     ]
    }
   ],
   "source": [
    "# 处理预测数据\n",
    "predict_data_sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "#     print(row['title'], row['content'])\n",
    "    title = row['title']\n",
    "    content = row['content']\n",
    "    preprocess_text(title, content, predict_data_sentences, label= None)\n",
    "\n",
    "x, none_labels = zip(*predict_data_sentences)\n",
    "# vec.fit(x)\n",
    "x = vec.transform(x)\n",
    "\n",
    "# print(x)\n",
    "# 应用模型进行预测\n",
    "result=mlp_model.predict(x)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'style' from 'pyecharts.charts' (/home/yang/anaconda3/lib/python3.7/site-packages/pyecharts/charts/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-249-272d75a2d1b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyecharts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# bar = Bar()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# bar.add_xaxis([\"衬衫\", \"羊毛衫\", \"雪纺衫\", \"裤子\", \"高跟鞋\", \"袜子\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bar.add_yaxis(\"商家A\", [5, 20, 36, 10, 75, 90])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'style' from 'pyecharts.charts' (/home/yang/anaconda3/lib/python3.7/site-packages/pyecharts/charts/__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyecharts.charts import Pie\n",
    "\n",
    "# bar = Bar()\n",
    "# bar.add_xaxis([\"衬衫\", \"羊毛衫\", \"雪纺衫\", \"裤子\", \"高跟鞋\", \"袜子\"])\n",
    "# bar.add_yaxis(\"商家A\", [5, 20, 36, 10, 75, 90])\n",
    "# bar.add_yaxis(\"商家B\", [6, 20, 26, 10, 65, 100])\n",
    "# # render 会生成本地 HTML 文件，默认会在当前目录生成 render.html 文件\n",
    "# # 也可以传入路径参数，如 bar.render(\"mycharts.html\")\n",
    "# bar.render_notebook()\n",
    "\n",
    "\n",
    "pos = 0\n",
    "neg = 0\n",
    "neu = 0\n",
    "\n",
    "for i in result:\n",
    "    if i == '正面':\n",
    "        pos += 1\n",
    "    elif i == '负面':\n",
    "        neg += 1\n",
    "    elif i == '中性':\n",
    "        neu += 1\n",
    "        \n",
    "res = [pos, neg, neu] \n",
    "print(res)\n",
    "pie = Pie(\"类别分析\",**style.init_style)\n",
    "\n",
    "attr = ['正面','负面','中性']\n",
    "\n",
    "pie.add(\"\",attr, res)\n",
    "\n",
    "# pie.add('类别信息', [pos, neg, neu], is_label_show=True, legend_pos='left', label_text_color=None, legend_orient='vertical', radius=[30, 75])\n",
    "\n",
    "#     pie.render('Pie-weather.html')\n",
    "pie.render_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
